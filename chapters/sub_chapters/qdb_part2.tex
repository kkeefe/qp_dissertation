\section{Frequency Calibration of Local Oscillators}~\label{sec:calib}

The Q-Pix calibration requirements are described in detail in Section~\ref{sec:qpix_circuit}.
The important parameters which must be calibrated for each pixel are the charge per reset and the frequency of the local oscillator.
An aim of this work is to demonstrate an additional frequency calibration method using the minimal required connections between each digital node.

Any method of a frequency calibration must synchronize time measurements between all digital nodes within a tile and the aggregator.
There are several possibile methods to achieve this, but ultimately the data that are recorded must be some time at the aggregator, $T_{a}$, and the time at any specific node, $T_{j}$.

%% distribute clock for calibration of time
A direct method is one where the aggregator distributes its own clock to all nodes in the tile.
This scenario removes the need for a calculation of the frequency of each node altogether since the clock of each node is already known from the aggregator.
This is the simplest case for timing calibration: remove all free running oscillators.

% cross-talk
A distributed clock network indeed removes ambiguity of the remote oscillator frequencies, but at the cost of hardware complexity.
Whether or not this design choice is preferred is entirely detector dependent, but likely increases in difficulty with the scale of the TPC.

We comment, however, that we ignore this scenario because it may altogether be unnecessary depending on future ASIC performance.
In the event that frequency calibrations of sufficient precision ($\bar{f} \approx 1 ppm$) are possible occur on free-running local oscillators future detectors would need only to acquire these ASICs and place them with minimal cost in terms of both time and money.

%% distribute trigger for calibration of time
Another simple scenario is one where the aggregator itself connects directly to all nodes within a tile via a single connection which can be used as a reference trigger.
This means that some trigger from the aggregator would issue directly into each node at the same time: $T_{a} = T_{n}$.
To calcuate the frequency in this manner, the controller would issue two triggers from the aggregator with a known time separation, $T_{o} = T_{a2} - T_{a1}$.
The remote nodes would each record and send their timestamps back to the aggregator, where the time difference would be calculated as:

\begin{equation}
  T_{o} = T_{a2} - T_{a1} = T_{n2} - T_{n1}
\end{equation}

this is rewritten in terms of frequency as follows:
\begin{equation}
  f_{n} = \frac{T_{n2} - T_{n1}}{T_{o}}
\end{equation}

This calibration method, though extremely simple, introduces an additional connection to each ASIC between itself and the aggregator.
For a large scale system such as Q-Pix a single connection per ASIC introduces $\approx 60\times 10^{3}$ hardware points of failure per APA.

Both of these scenarios are valid implementations of a Q-Pix readout system.
In both of these scenarios, however, there is added complexity into the hardware design of the system in the form of additional routing where each route which represents a possible point of failure.

In a world of perfect hardware and costless routing in terms of both time and money these routing schemes would clearly be sufficient.
However, no hardware is perfect.
Therefore we introduce and discuss a calibration technique which relies on no additional routing and could be optionally implemented even in the above schemes in the event of a failure.
Therefore, even if not the primary implemented calibration technqiue, since this calibration introduces no superfluous routing it could still be used regardless of the actual future hardware implementation.

%% distribute packet for calibration of time
\subsection{A Minimal Connection Calibration Procedure}~\label{sec:min_calib}

As stated previously, any frequency calibration records a reference time at the aggregator ($T_{a}$) and an event time ($T_{n}$) at a ASIC within a tile.
The time calibration procedure presented here requires only the minimal routing required in any Q-Pix readout system, where we assume time-dependent free-running local oscillators at each ASIC within the tile.

%% issue 1, then recv1
The calibration procedure begins at a time ($T_{0}$) where the aggregator sends a calibration packet.
Next, the packet propagates through the tile to some remote ASIC, $N_{j}$.
This ASIC receives the packet later at some time $T_{n1}$:

\begin{equation}
  T_{n1} = T_{o} + T_{f1}
\end{equation}


%% meas1 -> wait
Where $T_{f1}$ is the propogation time of the packet from the aggregator to the $N_{j}$ ASIC.
This remote ASIC then sends the packet with its time ($T_{n1}$) back to the aggregator.
The aggregator will wait some calibration time ($T_{cal}$) before issuing another calibration packet.
This wait period $(\mathcal{O}(10^{0-2})$) can be long compared to the full transaction time to the $N_{j}$ ASIC $(\mathcal{O}(j*10^{-5})$).

%% issue 1
After the wait period, the aggregator will issue a second calibration packet to be sent to a remote ASIC at time:
\begin{equation}~\label{eq:calibration_wait_time}
  T_{1} = T_{cal} + T_{0}
\end{equation}

%% recv2 and meas2
Similarly to the first packet this packet will propagate to $N_{j}$ with some new time $T_{f2}$ where $N_{j}$ will record time $T_{n2}$:
\begin{equation}
  T_{n2} = T_{1} + T_{f2}
\end{equation}

Now, we define $\Delta T_{j}$ as the difference in the two time measurements from the two packets sent from the aggregator.
The time difference is related to the number of clocks that occured between the two different measured values of the clock, $T_{n1}$ and $T_{n2}$.

\begin{equation}
  \Delta T_{j} = T_{n2} - T_{n1}
\end{equation}

We use the known relationships for $T_{n2}$ and $T_{n1}$ to obtain:
\begin{equation}
  \Delta T_{j} = (T_{1} + T_{f2}) - (T_{o} + T_{f1}) = (T_{1} - T_{0}) + (T_{f2} - T_{f1}) = T_{cal} + \Delta T_{f}
\end{equation}

Where we defined $\Delta T_{f}$ as the difference in forward propagation times from the packets sent from the aggregator at $T_{1}$ and $T_{0}$.

We arrive at the result which compares the measured time at the aggregator $T_{cal}$ and the time measured at each ASIC, $\Delta T_{j}$:
\begin{equation}
  \Delta T_{j} = T_{cal} + \Delta T_{f}
\end{equation}

A perfect reconstruction of the nodal frequency would follow if $\Delta T_{f} = 0$.
But it is sufficient to note that the wait period happens on the order of seconds, whereas $\Delta T_{f}$ is on the order of $\mu s$ or at least a six order of magnitude difference.
We then use $\Delta T_{f} \ll T_{cal}$ to obtain:
\begin{equation}
  \Delta T_{j} \approx T_{cal}
\end{equation}

We convert time into frequency with the difference of the timestamps measured and a known aggregator frequency ($f_{a}$):
\begin{equation}
   \frac{\Delta N_{j}}{f_{j}} = \frac{\Delta N_{a}}{f_{a}}
\end{equation}

or,
\begin{equation}~\label{eq:frequency_reconstruction}
   \boxed{f_{j} \simeq \frac{\Delta N_{j}}{\Delta N_{a}}f_{a}}
\end{equation}

Where $\Delta N_{j}$ and $\Delta N_{a}$ are the differences in the timestamps of the 32-bit clocks at the remote node and aggregator, respectively.


\subsubsection{Packet Transaction Time}

We next examine the approximation that $\Delta T_{f} \ll T_{cal}$ and consider its contribution to the error in the reconstruction of $T_{j}$ in Equation~\ref{eq:frequency_reconstruction}.
This analysis also provides a constraint on the duration of $T_{cal}$ (Equation~\ref{eq:calibration_wait_time}) to ensure an accurate measurement of each $T_{j}$ in a tile.
We begin by discussing how long it takes for a packet to traverse a tile.

The time it takes for each packet to be received by the next node is given in Equation~\ref{eq:t_packet}.
The value, $N_{bit}$, is the number of clock cycles used for the packet and is protocol-dependent.
Since the protocol must be deterministic for each packet, $N_{bits}$ must be the same for each transaction on the path from the base-node to the remote node.
As an example, the time it takes for a packet to go from the base-node, $N_{1}$, to a remote node, $N_{3}$, via the path $1\rightarrow 2 \rightarrow 3$ is determined by:
%% packet transaction time
\begin{equation}~\label{eq:t_packetTransfer}
  T_{1\rightarrow 3} = T_{1\rightarrow 2} + T_{2\rightarrow 3} \approx \frac{N_{bits}}{f_{1}} + \frac{N_{bits}}{f_{2}} = N_{bits}(\frac{1}{f_{1}} + \frac{1}{f_{2}})
\end{equation}

Where, $f_{i}$, is the frequency of the clock at sending node. 
he approximation is within a single clock cycle of the receiving digital node ($\approx 33~\unit{ns}$).
Therefore the time it takes for a packet for go from the base-node to any remote node is proportional to $N_{bits}$ multiplied by the sum of the edges in the full adjacency matrix given by Equation~\ref{eq:adjacency_comp}.

We generalize Equation~\ref{eq:t_packetTransfer} to represent the time it takes a packet to go from the aggregator ($i = 0$) to any remote node, $N_{j}$:
\begin{equation}
  T_{f} = T_{0\rightarrow j} = N_{bits}\sum_{i=0}^{i=j-1}\frac{1}{f_{i}}
\end{equation}

We require that every calibration packet on the protocol uses the same number of clocks ($N_{bits}$ is the same for each identical Register Request, Section~\ref{sec:registers}).
$\Delta T_{f}$ becomes:
\begin{equation}
  \Delta T_{f} = N_{bits}\sum_{i=0}^{i=j-1}\frac{1}{\Delta f_{i}} = N_{bits} \sum_{i=0}^{i=j-1}\Delta T_{i}
\end{equation}

We recognize $\Delta T_{i}$ as the nominal time-dependent clock drift of the each local oscillator in the path between the base-node to the remote-node.
We can provide an order of magnitude estimate for $\Delta T_{f}$ if we assume a (poor) $\approx 5\%$ drift in each of the remote clocks within the tile during a period of $T_{cal} \approx 1~\unit{s}$.
In this approximation we also assume that the mean of the periods of the nodes are the designed value ($\approx 33~\unit{ns}$) for which a 5\% error gives $\sigma_{T_{f}} \approx 15~\unit{ps}$.
If we further assume that all of the clocks drift have error which drifts in the same direction (the mean drift doesn't cancel) then for 31 (16$\times$16 tile broadcast) transactions with 2000 clocks per transaction (Equation~\ref{eq:avg_packet}), we obtain for $\Delta T_{f}$:
\begin{equation}~\label{eq:calib_constraints}
  \Delta T_{f} \approx 2000 * 31 * 15\times 10^{-12} \approx 0.92~\unit{\mu s} 
\end{equation}

Equation~\ref{eq:calib_constraints} provides an estimatation of the timing uncertainty introduced by the calibraton scheme in a worst case 16$\times$16 tile scenario.
In the next section we introduce a prototype modular prototype board which was designed to test the stability of the communication protocol and the frequency calibration procedure presented here.

\section{The Q-Pix Digital Board}~\label{sec:qdb_prototype}

We describe the design of a modular digital back-end prototype board and discuss first results of the Q-Pix digital logic in an Field Programmable Gate Array (FPGA).
The results presented here constructed a tile 4$\times$4 array of Lattice iCE40UP5K-UWG30ITR FPGAs.
These FPGAs were chosen because of their small form factor, availability and pin out.
Each FPGA is programmed with the logic described in the previous sections based on the Q-Pix digital ASIC prototype.

The FPGAs are used to test the cotrol logic, communication stability, buffer requirements, and calibration procedure which will be tested in the future Q-Pix digital prototype ASIC.
The most important quantity to be calibrated for the digital nodes is the frequency of the local oscillator.
The Q-Pix reconstruction for both time and z position are dependent on this parameter (see Chapter~\ref{chap:qpix}).

An example block diagram of the prototype board (QDB) is shown in Figure~\ref{fig:qdb_diagram}.
Each PCB connects 4 FPGAs in a 2$\times$2 array.
In the diagram shown, FPGA-A can connect to FPGA-B and FPGA-D, but not FPGA-C.
The external right angle bracket connectors are used to interconnect multiple tiles for a modular design.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/qdb_layout_altium.png}
  \caption{Q-Pix Digital Board (QDB) Design}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/qdb_closeup.jpg}
  \caption{Q-Pix Digital Board (QDB) PCB}
\end{subfigure}
\caption{}
\label{fig:qdb_diagram}
\end{figure}

Future implementations of the digital back-end for Q-Pix may, of course, use different oscillators.
However, these results are still beneficial as a proof of concept for the frequency calibration, as well as tests to the packet loss susceptibility.
Packet loss is a function of relative frequency drift between neighbor nodes.

Each iCE40 FPGA is uses an 12~\unit{MHz} external oscillator~\citep{ecs1612mv}, which is then connected to a phase-locked loop (PLL).
The PLL uses a multiply by five and divide by three circuit to achieve the desired 30~\unit{MHz} average frequency.
The PLL is a special IP for the ice40UP5k FPGA's~\citep{latticeice40up}.

FPGA's are labeled A through D on each QDB, beginning in the "top-right" of the board and proceeding counter-clock wise, as shown in Figure~\ref{fig:qdb_diagram}.
Each FPGA has access to two right-angle connectors, which allow the QDBs to be interconnetected into a larger tile as shown in Figure~\ref{fig:qdb_test_setup}.

We use the Zybo-Z7 prototype board from digilent as the aggregator node.
Communication to and from the board uses a custom software interface that allows data communication between the Zybo and a PC.
This DAQ is nearly identical to the procedure used in the SAQ experiment in Chapter~\ref{chap:saq}.

\begin{figure}[]
\centering
\includegraphics[width=0.8\textwidth]{./images/qdb_frequency_test_setup.jpg}
\caption{Layout of a Zybo-Z7 aggregator node connected to a single FPGA in an interconnected 2$\times$2 QDB Tile.
The total amount of interconnected FPGAs for the tile size is 4$\times$4.
The FPGA that connects to the Zybo is FPGA-A as shwon in Figure~\ref{fig:qdb_diagram} on Board-A.
Not shown is the ethernet connection from the Zybo back to the controlling PC.
Some QDB boards are connected via jumper wires to allow easier programming of the FPGAs and for probbing pins.
}
\label{fig:qdb_test_setup}
\end{figure}

One of the main objectives of the prototype boards is to verify the communication protcol (Section~\ref{sec:comms}).
An example of a broadcast transaction between the Zybo and two FPGAs on a single QDB is shown in Figure~\ref{fig:example_broadcast}.
The transaction depicted is a broadcast register write, written to the command register (Table~\ref{table:node_registers}).
From left to right in image A, the packets follow this pattern:
The Zybo constructs a packet based on the protocol read from the 

\begin{enumerate}
  \item Zybo Sends Broadcast Register Write (hard interrogation)
  \item FPGA-A Forwards and responds to broadcast and responds
  \item FPGA-B Responds to broadcast 
  \item FPGA-A routes the remote packet to the Zybo
\end{enumerate}

\begin{figure}
\centering
\begin{subfigure}{0.475\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/qdb_example_packet_waveform.png}
  \caption{Example of a Hard interrogation.}
\end{subfigure}%
\begin{subfigure}{0.475\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/qdb_example_packet_waveform_diagram.png}
  \caption{Communication Route on Prototype}
\end{subfigure}
\caption{Timescale is 50 $\unit{\mu s}$.}
\label{fig:example_broadcast}
\end{figure}

\section{Prototype Frequency Calibration Results}~\label{sec:freq_calib_results}


\begin{figure}[]
\centering
\includegraphics[width=0.9\textwidth]{images/(0,0).pdf}
\caption{Example of frequency calibrations for the FPGA Adjacent to the Zybo.
Shown is the reconstructed frequency for the base node FPGA at (0,0) within the QDB tile.
}
\label{fig:frq_recon_node00}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[width=0.9\textwidth]{images/(3,3).pdf}
\caption{Reconstructed frequency of the FPGA at position (3,3) within the QDB tile.
The relative drifts of the frequencies shown for the more distance FPGA are greater than the base node (0,0), which is only one transaction again from the Zybo aggregator.
}
\label{fig:frq_recon_node33}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[width=0.9\textwidth]{images/fast_example.pdf}
\caption{Fast calibration procedure results.}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[width=0.9\textwidth]{images/slow_example.pdf}
\caption{Slow calibration example.}
\end{figure}

%% 4Hz interrogation calibration 
\input{chapters/tables/fpga_fit.tex}


%% 0.1 Hz interrogation calibration 
\input{chapters/tables/fpga_slow_fit.tex}

To verify the reliability of the communication protocol we can estimate the number of total successful transactions in each run.
Each FPGA sends a broadcast register command, plus the event end word as a response.
Each hard-interrogation then creates at least 32 successful packet transactions. 
The total number of interrogations between both frequency tests was over 7000.
This corresponds to at least 224,000 succesful packet transactions.

Also, performed were random register writes followed by read requests perform on each FPGA to the channel mask register (See Table~\ref{table:node_registers}).
The channel mask register is a proxy scratch register, since it allows the reading and writing of any 16 bits.
We performed a long run ($\approx$ 10~\unit{hours}) run where the Zybo would write to a random FPGA a random sequence of 16 bits, followed by a read.
A check would be performed on any mismatch between the read and write words.
This read and write sequence occured every 0.25 seconds, or 4~\unit{Hz}.
This test valids another $\approx 3600\cdot 10\cdot 4\cdot 2 = 288,000$ packets.
In total, an estimated 512,000 packets have been successfully sent without loss on the QDBs.

\subsection{Timing Stability and Communication Verification}~\label{sec:timing_test_results}

We describe here the methods of measuring a stable oscillator frequency for the prototype PCB based on the frequency calibration scheme developed in Section~\ref{sec:calib}.
We also comment on the results of the timing with resepect to the minimum required timing sensitivity in order to have accurate timestamp reconstruction.

\begin{figure}[]
\centering
\includegraphics[width=0.8\textwidth]{images/interrogation_ppm_diff.pdf}
\caption{Shown are the relative values of the frequency part-per-million (ppm) values from the frequency calibration procedure in both the 4~\unit{Hz} (orange) and 0.1~\unit{Hz} tests.
The x-axis represents the number of transactions required to reach the FPGA.
These data are taken from Tables~\ref{tab:fpga_calibration} and~\ref{tab:fpga_slow_calibration}.
Clearly indicated is the relationship between the interrogation rate: a faster interrogation leads to a sharper average time reconstruction.
Also shown is that the further each FPGA is from the base-node, the worse the frequency reconstruction is.
This happens since there are more average clock cycles tranversed to reach the remote ASIC (see Equation~\ref{eq:calib_constraints}).
}
\end{figure}


\section{Towards the Integration of the Aggregator Node}

In the studies presented here, The aggregator node which was used was the Zybo Z7-20.

\section{Comments on A Super-DAQ-Node}

Each APA module within a larger DUNE module must ultimately be interconnected so that the entire module can be readout.
As described above, a single modular tile is controlled by an individual DAQ node, where many constitute a complete APA.
Therefore, we refer to the device that digitally multiplexes all of the DAQ node data as the "Super DAQ Node" (SDN).
Then, we imagine the final multiplexing stage for an entire DUNE module as an array of SDNs, each of which consistute an array of DAQ nodes, where each DAQ node is a 2-D array of Q-Pix based ASICs.

The total number of request SDNs within the full dune module depends on the final size of a DAQ-node controlled tile.

%% high level figure here from TPC -> Integrator -> Digital Node -> Aggregator -> SuperDAQ-Node -> WIC -> Disc

\section{The Back-End Summary}
